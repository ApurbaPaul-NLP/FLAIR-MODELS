{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoGh0HVTRAdlB/ZYLhncO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ApurbaPaul-NLP/FLAIR-MODELS/blob/main/Prog3_07_09_2022_Loading_Training_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUbFxKg715lp",
        "outputId": "c552c562-655b-4b10-bbe1-90810415dfa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 336 kB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.9.1)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.4.0)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.64.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2022.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.12.1+cu113)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from flair) (8.14.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.2 MB/s \n",
            "\u001b[?25hCollecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 49.9 MB/s \n",
            "\u001b[?25hCollecting conllu>=4.0\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.10)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.21.6)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.7.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair) (0.16.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (6.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Building wheels for collected packages: mpld3, overrides, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=285e6d604f5ccdc6a906614d36240213c3a838b873c046026f0328f0c467039d\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=d1e0bc941060247292aee2c6887d1c040927e069b17e02c6d7061f6249d206b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=b6404bf731f32f2ed6d20f48e993001e69bc9977b4410a75eac7a4a18f298cf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=6f77af899198af04ecf5c6eb7f78b5ac9ae63570dde83ff122b988d58738373a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=a089190f68042996d13265b50bf8b31345168113b9e130e1a9502da99b345e46\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=fa12e5485ef539aaa7ea765c1b2a18ea7fe17b1b8bbce0a9e826b4746f7ef87e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built mpld3 overrides sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, importlib-metadata, tokenizers, sentencepiece, py4j, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.12.0\n",
            "    Uninstalling importlib-metadata-4.12.0:\n",
            "      Successfully uninstalled importlib-metadata-4.12.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 huggingface-hub-0.9.1 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 requests-2.28.1 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tokenizers-0.12.1 transformers-4.21.3 wikipedia-api-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Corpus Object**"
      ],
      "metadata": {
        "id": "ParHV3B8JxsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Corpus represents a dataset that you use to train a model. \n",
        "\n",
        "It consists of a list of train sentences, a list of dev sentences, and a list of test sentences, which correspond to the training, validation and testing split during model training.\n",
        "\n",
        "The following example snippet instantiates the Universal Dependency Treebank for English as a corpus object:"
      ],
      "metadata": {
        "id": "O3A0QibqKEaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flair.datasets\n",
        "corpus = flair.datasets.UD_ENGLISH()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs_8-hiqKIMy",
        "outputId": "2ea350bf-b9f8-4415-d58b-e4c565643da5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:35,483 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu not found in cache, downloading to /tmp/tmpxut8yj9t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1738438B [00:00, 64337284.44B/s]         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:35,558 copying /tmp/tmpxut8yj9t to cache at /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2022-09-06 23:35:35,563 removing temp file /tmp/tmpxut8yj9t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:36,318 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu not found in cache, downloading to /tmp/tmpqniotv4f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1738935B [00:00, 61981066.72B/s]         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:36,410 copying /tmp/tmpqniotv4f to cache at /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "2022-09-06 23:35:36,416 removing temp file /tmp/tmpqniotv4f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:37,857 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu not found in cache, downloading to /tmp/tmptpx0_el2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13686411B [00:00, 101458594.52B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:35:38,039 copying /tmp/tmptpx0_el2 to cache at /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2022-09-06 23:35:38,057 removing temp file /tmp/tmptpx0_el2\n",
            "2022-09-06 23:35:38,061 Reading data from /root/.flair/datasets/ud_english\n",
            "2022-09-06 23:35:38,063 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2022-09-06 23:35:38,066 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2022-09-06 23:35:38,069 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first time you call this snippet, it triggers a download of the Universal Dependency Treebank for English onto your hard drive. \n",
        "\n",
        "It then reads the train, test and dev splits into the Corpus which it returns. \n",
        "\n",
        "Check the length of the three splits to see how many Sentences are there:"
      ],
      "metadata": {
        "id": "QctQJGvTKXum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the number of Sentences in the train split\n",
        "print(len(corpus.train))\n",
        "\n",
        "# print the number of Sentences in the test split\n",
        "print(len(corpus.test))\n",
        "\n",
        "# print the number of Sentences in the dev split\n",
        "print(len(corpus.dev))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT6tFyyvKaJ0",
        "outputId": "a7621f6f-d4fc-49b7-cbee-a3273593c937"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12543\n",
            "2077\n",
            "2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also access the Sentence objects in each split directly. \n",
        "\n",
        "For instance, let us look at the first Sentence in the training split of the English UD:"
      ],
      "metadata": {
        "id": "-osZHQqgKffg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first Sentence in the training split\n",
        "sentence = corpus.test[0]\n",
        "\n",
        "# print with all annotations\n",
        "print(sentence)\n",
        "\n",
        "# print only with POS annotations (better readability)\n",
        "print(sentence.to_tagged_string('pos'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZiFhSPYKjJR",
        "outputId": "e12952ba-9d98-4968-e090-ed2c764c289c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"What if Google Morphed Into GoogleOS ?\" → [\"What\"/what/PRON/WP/root/Int, \"if\"/if/SCONJ/IN/mark, \"Google\"/Google/PROPN/NNP/nsubj/Sing, \"Morphed\"/morph/VERB/VBD/advcl/Ind/Sing/3/Past/Fin, \"Into\"/into/ADP/IN/case, \"GoogleOS\"/GoogleOS/PROPN/NNP/obl/Sing, \"?\"/?/PUNCT/./punct]\n",
            "Sentence: \"What if Google Morphed Into GoogleOS ?\" → [\"What\"/WP, \"if\"/IN, \"Google\"/NNP, \"Morphed\"/VBD, \"Into\"/IN, \"GoogleOS\"/NNP, \"?\"/.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions**"
      ],
      "metadata": {
        "id": "ynF2zAktK3-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Corpus contains a bunch of useful helper functions. \n",
        "\n",
        "For instance, you can downsample the data by calling downsample() and passing a ratio. \n",
        "\n",
        "So, if you normally get a corpus like this:"
      ],
      "metadata": {
        "id": "6ffXs317K66Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flair.datasets\n",
        "corpus = flair.datasets.UD_ENGLISH()\n",
        "downsampled_corpus = flair.datasets.UD_ENGLISH().downsample(0.1) #you can downsample the corpus, simply like this.\n",
        "#If you print both corpora, you see that the second one has been downsampled to 10% of the data.\n",
        "print(\"--- 1 Original ---\")\n",
        "print(corpus)\n",
        "\n",
        "print(\"--- 2 Downsampled ---\")\n",
        "print(downsampled_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD471hl3K9BC",
        "outputId": "3bc6a2d2-7d36-4457-be3b-4d1664f6ed4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:40:14,405 Reading data from /root/.flair/datasets/ud_english\n",
            "2022-09-06 23:40:14,408 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2022-09-06 23:40:14,410 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2022-09-06 23:40:14,411 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "2022-09-06 23:40:36,687 Reading data from /root/.flair/datasets/ud_english\n",
            "2022-09-06 23:40:36,688 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2022-09-06 23:40:36,693 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2022-09-06 23:40:36,698 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "--- 1 Original ---\n",
            "Corpus: 12543 train + 2001 dev + 2077 test sentences\n",
            "--- 2 Downsampled ---\n",
            "Corpus: 1254 train + 200 dev + 208 test sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating label dictionaries**"
      ],
      "metadata": {
        "id": "5Ypdin9ALjiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For many learning tasks you need to create a \"dictionary\" that contains all the labels you want to predict. \n",
        "\n",
        "You can generate this dictionary directly out of the Corpus by calling the method make_label_dictionary and passing the desired label_type.\n",
        "\n",
        "For instance, the UD_ENGLISH corpus instantiated above has multiple layers of annotation like regular POS tags ('pos'), universal POS tags ('upos'), morphological tags ('tense', 'number'..) and so on. \n",
        "\n",
        "Create label dictionaries for universal POS tags by passing label_type='upos' like this:"
      ],
      "metadata": {
        "id": "gow1ZBjLLup7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create label dictionary for a Universal Part-of-Speech tagging task\n",
        "upos_dictionary = corpus.make_label_dictionary(label_type='upos')\n",
        "\n",
        "# print dictionary\n",
        "print(upos_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_TNV-CTLknD",
        "outputId": "82e99a01-5cfe-41de-f79f-280b9a84e6c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:42:42,509 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12543it [00:00, 19772.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:42:43,196 Dictionary created for label 'upos' with 18 values: NOUN (seen 34761 times), PUNCT (seen 23620 times), VERB (seen 22946 times), PRON (seen 18589 times), ADP (seen 17730 times), DET (seen 16314 times), ADJ (seen 13167 times), AUX (seen 12440 times), PROPN (seen 12345 times), ADV (seen 9462 times), CCONJ (seen 6690 times), PART (seen 5745 times), SCONJ (seen 4554 times), NUM (seen 4119 times), X (seen 704 times), SYM (seen 698 times), INTJ (seen 694 times)\n",
            "Dictionary with 18 tags: <unk>, NOUN, PUNCT, VERB, PRON, ADP, DET, ADJ, AUX, PROPN, ADV, CCONJ, PART, SCONJ, NUM, X, SYM, INTJ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tense_dictionary = corpus.make_label_dictionary(label_type='tense')\n",
        "\n",
        "# print dictionary\n",
        "print(tense_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKqiMcKEMCgO",
        "outputId": "d2d7640c-9a77-42d7-b4ca-0db91e7d169d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:44:02,263 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12543it [00:00, 40272.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:44:02,581 Dictionary created for label 'tense' with 3 values: Pres (seen 10870 times), Past (seen 9357 times)\n",
            "Dictionary with 3 tags: <unk>, Pres, Past\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dictionaries for other label types**"
      ],
      "metadata": {
        "id": "5xwxs36UMQCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't know the label types in a corpus, just call make_label_dictionary with any random label name (e.g. corpus.make_label_dictionary(label_type='abcd')). This will print out statistics on all label types in the corpus:"
      ],
      "metadata": {
        "id": "WZPCOhBVMUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.make_label_dictionary(label_type='all')  #here in label_type anything you can write to check all label_types. Obviously it will show an error msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "ZkCYfiFnMVQ1",
        "outputId": "d76749a3-0cd9-4b4e-8910-8199b64d2b57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:47:13,794 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12543it [00:00, 50241.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:47:14,054 ERROR: You specified label_type='all' which is not in this dataset!\n",
            "2022-09-06 23:47:14,056 ERROR: The corpus contains the following label types: 'lemma' (in 12543 sentences), 'upos' (in 12543 sentences), 'pos' (in 12543 sentences), 'dependency' (in 12543 sentences), 'number' (in 12037 sentences), 'verbform' (in 10123 sentences), 'prontype' (in 9750 sentences), 'person' (in 9387 sentences), 'mood' (in 8911 sentences), 'tense' (in 8747 sentences), 'degree' (in 7149 sentences), 'definite' (in 6851 sentences), 'case' (in 6492 sentences), 'gender' (in 2829 sentences), 'numtype' (in 2771 sentences), 'poss' (in 2537 sentences), 'voice' (in 1085 sentences), 'typo' (in 553 sentences), 'extpos' (in 185 sentences), 'abbr' (in 177 sentences), 'reflex' (in 134 sentences), 'style' (in 48 sentences), 'foreign' (in 5 sentences)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f81e08000b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flair/data.py\u001b[0m in \u001b[0;36mmake_label_dictionary\u001b[0;34m(self, label_type, min_count)\u001b[0m\n\u001b[1;32m   1485\u001b[0m             )\n\u001b[1;32m   1486\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: The corpus contains the following label types: {contained_labels}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         log.info(\n",
            "\u001b[0;31mException\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create dictionaries for regular part of speech tags and a morphological number tagging task:"
      ],
      "metadata": {
        "id": "UZE6YLl4NGek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create label dictionary for a regular POS tagging task\n",
        "pos_dictionary = corpus.make_label_dictionary(label_type='pos')\n",
        "print(pos_dictionary)\n",
        "# create label dictionary for a morphological number tagging task\n",
        "tense_dictionary = corpus.make_label_dictionary(label_type='number')\n",
        "print(tense_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUFtIk4dNIgQ",
        "outputId": "79634686-978f-44d8-c329-ac9a73dcfe6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:49:31,938 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12543it [00:00, 19977.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:49:32,575 Dictionary created for label 'pos' with 51 values: NN (seen 26920 times), IN (seen 20882 times), DT (seen 16845 times), NNP (seen 12401 times), PRP (seen 12220 times), JJ (seen 11587 times), RB (seen 10592 times), . (seen 10317 times), VB (seen 9487 times), NNS (seen 8450 times), , (seen 8062 times), CC (seen 6693 times), VBD (seen 5400 times), VBP (seen 5349 times), VBZ (seen 4569 times), CD (seen 4002 times), VBN (seen 3957 times), VBG (seen 3330 times), MD (seen 3292 times), TO (seen 3283 times)\n",
            "Dictionary with 51 tags: <unk>, NN, IN, DT, NNP, PRP, JJ, RB, ., VB, NNS, ,, CC, VBD, VBP, VBZ, CD, VBN, VBG, MD, TO, PRP$, -RRB-, -LRB-, WDT, :, WRB, ``, '', WP, RP, UH, POS, HYPH, NNPS, JJR, JJS, EX, NFP, RBR, ADD, GW, $, PDT, SYM, LS, RBS, FW, AFX, WP$\n",
            "2022-09-06 23:49:32,578 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "12543it [00:00, 29369.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:49:33,015 Dictionary created for label 'number' with 3 values: Sing (seen 60903 times), Plur (seen 16416 times)\n",
            "Dictionary with 3 tags: <unk>, Sing, Plur\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dictionaries for other corpora types**"
      ],
      "metadata": {
        "id": "Q-CmhdItNpOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method make_label_dictionary can be used for any corpus, including text classification corpora:"
      ],
      "metadata": {
        "id": "ZketFzdeNsG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create label dictionary for a text classification task\n",
        "corpus = flair.datasets.TREC_6()\n",
        "# corpus.make_label_dictionary(label_type='all') #The corpus contains the following label types: 'question_class' (in 4907 sentences). It will show an error msg\n",
        "corpus.make_label_dictionary('question_class')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLQfU_OoNvvJ",
        "outputId": "470b2434-974b-43f4-f250-2cbd97b143c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:52:44,860 Reading data from /root/.flair/datasets/trec_6\n",
            "2022-09-06 23:52:44,861 Train: /root/.flair/datasets/trec_6/train.txt\n",
            "2022-09-06 23:52:44,865 Dev: None\n",
            "2022-09-06 23:52:44,866 Test: /root/.flair/datasets/trec_6/test.txt\n",
            "2022-09-06 23:52:45,447 Initialized corpus /root/.flair/datasets/trec_6 (label type name is 'question_class')\n",
            "2022-09-06 23:52:45,449 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4907it [00:00, 51206.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:52:45,559 Dictionary created for label 'question_class' with 7 values: ENTY (seen 1103 times), HUM (seen 1088 times), DESC (seen 1057 times), NUM (seen 817 times), LOC (seen 764 times), ABBR (seen 78 times)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<flair.data.Dictionary at 0x7f1fe8118810>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The MultiCorpus Object**"
      ],
      "metadata": {
        "id": "U-7jGNpqORIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to train multiple tasks at once, you can use the MultiCorpus object. \n",
        "\n",
        "To initiate the MultiCorpus you first need to create any number of Corpus objects. \n",
        "\n",
        "Afterwards, you can pass a list of Corpus to the MultiCorpus object. \n",
        "\n",
        "For instance, the following snippet loads a combination corpus consisting of the English, German and Dutch Universal Dependency Treebanks.\n",
        "\n",
        "The MultiCorpus inherits from Corpus, so you can use it like any other corpus to train your models."
      ],
      "metadata": {
        "id": "k-DC-p99OSVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_corpus = flair.datasets.UD_ENGLISH()\n",
        "german_corpus = flair.datasets.UD_GERMAN()\n",
        "dutch_corpus = flair.datasets.UD_DUTCH()\n",
        "\n",
        "# make a multi corpus consisting of three UDs\n",
        "from flair.data import MultiCorpus\n",
        "multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfyAdMQYOWfv",
        "outputId": "a1d8e733-9ec9-48aa-9f73-bcaab4d73e3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:54:43,993 Reading data from /root/.flair/datasets/ud_english\n",
            "2022-09-06 23:54:43,996 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2022-09-06 23:54:43,998 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2022-09-06 23:54:44,001 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "2022-09-06 23:55:12,275 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-dev.conllu not found in cache, downloading to /tmp/tmp3t7gzolw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "902796B [00:00, 53792292.90B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:12,353 copying /tmp/tmp3t7gzolw to cache at /root/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
            "2022-09-06 23:55:12,357 removing temp file /tmp/tmp3t7gzolw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:12,763 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-test.conllu not found in cache, downloading to /tmp/tmpeeea0kze\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1205132B [00:00, 68425904.19B/s]         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:12,824 copying /tmp/tmpeeea0kze to cache at /root/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
            "2022-09-06 23:55:12,827 removing temp file /tmp/tmpeeea0kze\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:14,596 https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master/de_gsd-ud-train.conllu not found in cache, downloading to /tmp/tmpb0mwdw9d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19591438B [00:00, 103732657.16B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:14,842 copying /tmp/tmpb0mwdw9d to cache at /root/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:14,868 removing temp file /tmp/tmpb0mwdw9d\n",
            "2022-09-06 23:55:14,874 Reading data from /root/.flair/datasets/ud_german\n",
            "2022-09-06 23:55:14,875 Train: /root/.flair/datasets/ud_german/de_gsd-ud-train.conllu\n",
            "2022-09-06 23:55:14,878 Dev: /root/.flair/datasets/ud_german/de_gsd-ud-dev.conllu\n",
            "2022-09-06 23:55:14,881 Test: /root/.flair/datasets/ud_german/de_gsd-ud-test.conllu\n",
            "2022-09-06 23:55:41,812 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-dev.conllu not found in cache, downloading to /tmp/tmpkb3e33fv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "965554B [00:00, 62084392.46B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:41,872 copying /tmp/tmpkb3e33fv to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-dev.conllu\n",
            "2022-09-06 23:55:41,877 removing temp file /tmp/tmpkb3e33fv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:42,229 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-test.conllu not found in cache, downloading to /tmp/tmpm6se4lpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "924462B [00:00, 58229083.41B/s]          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:42,285 copying /tmp/tmpm6se4lpu to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-test.conllu\n",
            "2022-09-06 23:55:42,293 removing temp file /tmp/tmpm6se4lpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:43,828 https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master/nl_alpino-ud-train.conllu not found in cache, downloading to /tmp/tmprnadjor1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "14864664B [00:00, 109544685.68B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-09-06 23:55:44,023 copying /tmp/tmprnadjor1 to cache at /root/.flair/datasets/ud_dutch/nl_alpino-ud-train.conllu\n",
            "2022-09-06 23:55:44,044 removing temp file /tmp/tmprnadjor1\n",
            "2022-09-06 23:55:44,048 Reading data from /root/.flair/datasets/ud_dutch\n",
            "2022-09-06 23:55:44,050 Train: /root/.flair/datasets/ud_dutch/nl_alpino-ud-train.conllu\n",
            "2022-09-06 23:55:44,053 Dev: /root/.flair/datasets/ud_dutch/nl_alpino-ud-dev.conllu\n",
            "2022-09-06 23:55:44,056 Test: /root/.flair/datasets/ud_dutch/nl_alpino-ud-test.conllu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading Your Own Sequence Labeling Dataset**"
      ],
      "metadata": {
        "id": "cAhL8jN0O1cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cases you want to train over a sequence labeling dataset that is not in the above list, you can load them with the ColumnCorpus object. \n",
        "\n",
        "Most sequence labeling datasets in NLP use some sort of column format in which each line is a word and each column is one level of linguistic annotation. \n",
        "\n",
        "See for instance this sentence:"
      ],
      "metadata": {
        "id": "TJCe38xmQT3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "George N B-PER\n",
        "Washington N I-PER\n",
        "went V O\n",
        "to P O\n",
        "Washington N B-LOC\n",
        "\n",
        "Sam N B-PER\n",
        "Houston N I-PER\n",
        "stayed V O\n",
        "home N O\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7PlYhpnVQXsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "The first column is the word itself, the second coarse PoS tags, and the third BIO-annotated NER tags. Empty line separates sentences. \n",
        "\n",
        "To read such a dataset, define the column structure as a dictionary and instantiate a ColumnCorpus.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "k7KZoO4sQbkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "# define columns\n",
        "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
        "\n",
        "# this is the folder in which train, test and dev files reside\n",
        "data_folder = '/path/to/data/folder'\n",
        "\n",
        "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file='train.txt',\n",
        "                              test_file='test.txt',\n",
        "                              dev_file='dev.txt')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-wzBCIiqQiGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "This gives you a Corpus object that contains the train, dev and test splits, each has a list of Sentence. \n",
        "\n",
        "So, to check how many sentences there are in the training split, do\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hAT6Ml-SQpHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "len(corpus.train)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "etHvNIPqQsKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "You can also access a sentence and check out annotations. \n",
        "\n",
        "Lets assume that the training split is read from the example above, then executing these commands\n",
        "\n",
        "print(corpus.train[0].to_tagged_string('ner'))\n",
        "print(corpus.train[1].to_tagged_string('pos'))\n",
        "\n",
        "\n",
        "will print the sentences with different layers of annotation:\n",
        "\n",
        "\n",
        "George <B-PER> Washington <I-PER> went to Washington <B-LOC> .\n",
        "\n",
        "Sam <N> Houston <N> stayed <V> home <N>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MiZldi12Q2Vz"
      }
    }
  ]
}